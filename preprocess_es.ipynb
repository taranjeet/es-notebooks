{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook is based on preprocess python notebook, It performs the same functionality but using es mapping\\n\\nHere we will be using analyzers, filters, char_filter to produce the same effect\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook is based on preprocess python notebook, It performs the same functionality but using es mapping\n",
    "\n",
    "Here we will be using analyzers, filters, char_filter to produce the same effect\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be preprocessed at two phases\n",
    "* Indexing phase\n",
    "* Searching phase\n",
    "\n",
    "##### Indexing phase\n",
    "\n",
    "When new docs are inserted into a index, this is called indexing phase\n",
    "\n",
    "##### Searching phase\n",
    "\n",
    "When any query is searched in a index, this is called searching phase\n",
    "\n",
    "For both the phases, we use custom analyzers which helps us in preprocessing the data. Analyzers in es consists of the following\n",
    "\n",
    "* Char Filters\n",
    "* Tokenizer\n",
    "* Filters\n",
    "\n",
    "A stream of text or text is initially passed to the char filter, which works at character level and performs any replacement/insertion/deletion, etc. Common examples can be replacing numbers with their string version, removing punctuation symbols, etc. After this stream of text is passed to tokenizer which tokenizes into tokens. Most common tokenizer is space tokenizer. After this, once we have the tokens, filters come into existence. They apply at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_HOST = 'http://localhost'\n",
    "ES_PORT = 9202\n",
    "ES_HOST_STRING = '{}:{}'.format(ES_HOST, ES_PORT)\n",
    "\n",
    "INDEX_NAME = 'movies_v3'\n",
    "TYPE_NAME = 'marvel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will first define settings for index and then at last create index\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {},\n",
    "                \"analyzer\": {},\n",
    "                \"char_filter\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first create a custom analyzer, so that we can add custom filter/char filters etc\n",
    "movie_analyzer_v1 = {\n",
    "    \"filter\": [],\n",
    "    \"char_filter\": [],\n",
    "    \"type\": \"custom\",\n",
    "    \"tokenizer\": \"standard\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"filter\": [\n",
      "    \"lowercase\"\n",
      "  ], \n",
      "  \"char_filter\": [], \n",
      "  \"type\": \"custom\", \n",
      "  \"tokenizer\": \"standard\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# lets add support for lowercase\n",
    "# lowercase exists as an inbuilt filter `lowercase`\n",
    "movie_analyzer_v1['filter'].append('lowercase')\n",
    "print json.dumps(movie_analyzer_v1, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"filter\": [\n",
      "    \"lowercase\"\n",
      "  ], \n",
      "  \"char_filter\": [\n",
      "    \"html_strip\"\n",
      "  ], \n",
      "  \"type\": \"custom\", \n",
      "  \"tokenizer\": \"standard\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# lets add support to remove html tags from text\n",
    "# `html_strip` exists as an inbuilt char filter\n",
    "movie_analyzer_v1['char_filter'].append('html_strip')\n",
    "print json.dumps(movie_analyzer_v1, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point if we pass any text through `movie_analyzer` it will be lowercase and does not contain any html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove punctuation symbols\n",
    "# for this there is no inbuild support in ES. we will have to create a custom `char_filter` for this\n",
    "remove_punctuation = {\n",
    "    'type': 'mapping',\n",
    "    'mappings': [\n",
    "        \"! =>\",\n",
    "        \"# =>\",\n",
    "        \"$ =>\",\n",
    "        \"% =>\",\n",
    "        \"& =>\",\n",
    "        \"' =>\",\n",
    "        \"( =>\",\n",
    "        \") =>\",\n",
    "        \"* =>\",\n",
    "        \"+ =>\",\n",
    "        \", =>\",\n",
    "        \"- =>\",\n",
    "        \". =>\",\n",
    "        \"/ =>\",\n",
    "        \": =>\",\n",
    "        \"; =>\",\n",
    "        \"< =>\",\n",
    "        \"= =>\",\n",
    "        \"> =>\",\n",
    "        \"? =>\",\n",
    "        \"@ =>\",\n",
    "        \"[ =>\",\n",
    "        \"] =>\",\n",
    "        \"^ =>\",\n",
    "        \"_ =>\",\n",
    "        \"` =>\",\n",
    "        \"{ =>\",\n",
    "        \"| =>\",\n",
    "        \"} =>\",\n",
    "        \"~ =>\",\n",
    "      ]\n",
    "}\n",
    "# add this in settings, so it can be used by any analyzer\n",
    "\n",
    "settings['settings']['index']['analysis']['char_filter']['remove_punctuation'] = remove_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"filter\": [\n",
      "    \"lowercase\"\n",
      "  ], \n",
      "  \"char_filter\": [\n",
      "    \"html_strip\", \n",
      "    \"remove_punctuation\"\n",
      "  ], \n",
      "  \"type\": \"custom\", \n",
      "  \"tokenizer\": \"standard\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# add this to `movie_analyzer`\n",
    "movie_analyzer_v1['char_filter'].append('remove_punctuation')\n",
    "print json.dumps(movie_analyzer_v1, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a mapping at this point and see if our changes are correctly applied or not\n",
    "settings['settings']['index']['analysis']['movie_analyzer_v1'] = movie_analyzer_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "  \"mappings\": {\n",
    "    \"review\": {\n",
    "      \"properties\": {\n",
    "        \"content\": {\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"id\": {\n",
    "          \"type\": \"long\"\n",
    "        },\n",
    "        \"sourceUrl\": {\n",
    "          \"type\": \"keyword\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([ES_HOST_STRING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"mappings\": {\n",
      "    \"review\": {\n",
      "      \"properties\": {\n",
      "        \"content\": {\n",
      "          \"type\": \"text\"\n",
      "        }, \n",
      "        \"sourceUrl\": {\n",
      "          \"type\": \"keyword\"\n",
      "        }, \n",
      "        \"id\": {\n",
      "          \"type\": \"long\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }, \n",
      "  \"settings\": {\n",
      "    \"index\": {\n",
      "      \"analysis\": {\n",
      "        \"filter\": {}, \n",
      "        \"char_filter\": {\n",
      "          \"remove_punctuation\": {\n",
      "            \"type\": \"mapping\", \n",
      "            \"mappings\": [\n",
      "              \"! =>\", \n",
      "              \"# =>\", \n",
      "              \"$ =>\", \n",
      "              \"% =>\", \n",
      "              \"& =>\", \n",
      "              \"' =>\", \n",
      "              \"( =>\", \n",
      "              \") =>\", \n",
      "              \"* =>\", \n",
      "              \"+ =>\", \n",
      "              \", =>\", \n",
      "              \"- =>\", \n",
      "              \". =>\", \n",
      "              \"/ =>\", \n",
      "              \": =>\", \n",
      "              \"; =>\", \n",
      "              \"< =>\", \n",
      "              \"= =>\", \n",
      "              \"> =>\", \n",
      "              \"? =>\", \n",
      "              \"@ =>\", \n",
      "              \"[ =>\", \n",
      "              \"] =>\", \n",
      "              \"^ =>\", \n",
      "              \"_ =>\", \n",
      "              \"` =>\", \n",
      "              \"{ =>\", \n",
      "              \"| =>\", \n",
      "              \"} =>\", \n",
      "              \"~ =>\"\n",
      "            ]\n",
      "          }\n",
      "        }, \n",
      "        \"movie_analyzer_v1\": {\n",
      "          \"filter\": [\n",
      "            \"lowercase\"\n",
      "          ], \n",
      "          \"char_filter\": [\n",
      "            \"html_strip\", \n",
      "            \"remove_punctuation\"\n",
      "          ], \n",
      "          \"type\": \"custom\", \n",
      "          \"tokenizer\": \"standard\"\n",
      "        }, \n",
      "        \"analyzer\": {}\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "index_body = {}\n",
    "index_body['settings'] = settings['settings']\n",
    "index_body['mappings'] = mappings['mappings']\n",
    "print json.dumps(index_body, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True, u'index': u'movies_v3', u'shards_acknowledged': True}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=INDEX_NAME, ignore=400, body=index_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets run `analyze` api to see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets consider review 1 (id 1)\n",
    "review_1_text = '''Moments that touch the heart are <strong>few</strong> and <strong>far</strong> between in this almost-culmination of a decade of Marvel Comics movies. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_query = {\n",
    "    'filter': ['lowercase'],\n",
    "    'text': review_1_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moments that touch the heart are <strong>few</strong> and <strong>far</strong> between in this almost-culmination of a decade of marvel comics movies. \n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(index=INDEX_NAME, body=analyze_query)\n",
    "print response['tokens'][0]['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that the text is now in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove html tags from review 1\n",
    "analyze_query = {\n",
    "    'tokenizer': 'standard', \n",
    "    'char_filter' : ['html_strip'],\n",
    "    'text': review_1_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moments that touch the heart are few and far between in this almost culmination of a decade of Marvel Comics movies\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(index=INDEX_NAME, body=analyze_query)\n",
    "for token_obj in response['tokens']:\n",
    "    print token_obj.get('token'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove punctuation symbols\n",
    "# lets consider review 3\n",
    "review_3_text = '''?Avengers: Infinity War? takes you places that most superhero movies don?t ? and where you may not want to go.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_query = {\n",
    "    'char_filter': ['remove_punctuation'],\n",
    "    'text': review_3_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers Infinity War takes you places that most superhero movies dont  and where you may not want to go\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(index=INDEX_NAME, body=analyze_query)\n",
    "print response['tokens'][0]['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how the punctuation symbols are removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
